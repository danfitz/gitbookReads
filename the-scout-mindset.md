---
title: "The Scout Mindset"
author: "Julia Galef"
---
  
# *The Scout Mindset*
`Author: Julia Galef`

**_Highlight_**

> Yes, we often rationalize away our mistakes—but we sometimes acknowledge them, too. We change our minds less often than we should but more often than we could. We’re complex creatures who sometimes hide the truth from ourselves and sometimes confront it. This book is about the less explored side of that coin, the times we succeed in not fooling ourselves, and what we can learn from those successes.
>  *(3.04183%)*

**_Highlight_**

> after several years of running workshops, reading studies, doing consulting, and interviewing people, I finally came to accept that knowing how to reason wasn’t the cure-all I thought it was. Knowing that you should test your assumptions doesn’t automatically improve your judgment, any more than knowing you should exercise automatically improves your health. Being able to rattle off a list of biases and fallacies doesn’t help you unless you’re willing to acknowledge those biases and fallacies in your own thinking. The biggest lesson I learned is something that’s since been corroborated by researchers, as we’ll see in this book: our judgment *(3.42205%)*

## The Case for Scout Mindset 

### Two Types of Thinking

**_Highlight_**

> Soldier Mindset   Scout Mindset       Reasoning is like defensive combat.   Reasoning is like mapmaking.     Decide what to believe by asking either “Can I believe this?” or “Must I believe this?” depending on your motives.   Decide what to believe by asking, “Is this true?”     Finding out you’re wrong means suffering a defeat.   Finding out you’re wrong means revising your map.     Seek out evidence to fortify and defend your beliefs.   Seek out evidence that will make your map more accurate.     Related concepts: Directionally motivated reasoning, rationalizing, denial, self-deception, wishful thinking   Related concepts: Accuracy motivated reasoning, truth-seeking, discovery, objectivity, intellectual honesty *(8.74525%)*

### What the Soldier Is Protecting 

**_Highlight_**

> when you advocate changing something, you should make sure you understand why it is the way it is in the first place. This rule is known as Chesterton’s fence, *(9.50570%)*

**_Highlight_**

> Soldier mindset helps us avoid negative emotions like fear, stress, and regret. *(9.88593%)*

**_Highlight_**

> 	 All sorts of beliefs can get drafted into service defending our egos because they relate in some way to our strengths or weaknesses.  *(10.26620%)*

**_Highlight_**

> 	 I wrote this book while living in San Francisco, the city where everyone and their Uber driver has a vision for the next billion-dollar tech company. Out here, it’s common wisdom that irrational optimism is a good thing—it’s what motivates you to launch yourself into daunting challenges, ignore the naysayers, and persevere when things get tough. *(10.64640%)*

**_Highlight_**

> The more we can get ourselves to genuinely believe those claims, and the more evidence and arguments we can collect to support them, the easier it will be for us to persuade other people of them (or so the logic goes).
>  *(11.40680%)*

**_Highlight_**

> When we’re picking out clothing to wear, deciding between suits or jeans, leather or hemp, high heels or high-tops, we implicitly ask ourselves: “What kind of person would wear this? Someone sophisticated, free-spirited, unconventional, down to earth? Is that how I want other people to see me?” We choose beliefs in a similar way.* Psychologists call it impression management, and evolutionary psychologists call it signaling: *(11.78710%)*

**_Note_**

> When you think about all of the things we use soldier mindset for, it becomes obvious why the frequently proposed fixes for it are futile. Such fixes typically involve words like “teaching” or “training,” as in:  We need to teach students about cognitive biases. We need to train people in critical thinking. We need to educate people in reason and logic.  None of these approaches have shown much promise in changing people’s thinking in the long run or outside of the classroom. And that should not surprise us. We use motivated reasoning not because we don’t know any better, but because we’re trying to protect things that are vitally important to us—our ability to feel good about our lives and ourselves, our motivation to try hard things and stick with them, our ability to look good *(12.92780%)*

* **Comments**: Hmmmmmmm

### Why Truth Is More Valuable Than We Realize 

**_Highlight_**

> 	 This is one of the paradoxes of being human: that our beliefs serve such different purposes all at once. Invariably, we end up making trade-offs.
>  *(13.68820%)*

**_Highlight_**

> 	 The hypothesis that the human mind evolved the ability to make these trade-offs well is called the rational irrationality hypothesis, coined by economist Bryan Caplan.1 *(14.44870%)*

**_Highlight_**

> two different senses of the word rational: epistemic rationality means holding beliefs that are well justified, while instrumental rationality means acting effectively to achieve your goals. Being rationally irrational, therefore, would mean that we’re good at unconsciously choosing just enough epistemic irrationality to achieve our social and emotional goals, without impairing our judgment too much. *(14.44870%)*

**_Highlight_**

> One of the most frustrating aspects of being human is our knack for undermining our own goals. We pay for gym memberships, then rarely use them. We start diets, then break them. We procrastinate on writing a paper until it’s the night before the deadline and we’re cursing our past self for putting us in this predicament. The source of this self-sabotage is present bias, a feature of our intuitive decision-making in which we care too much about short-term consequences and too little about long-term consequences. *(14.82890%)*

**_Highlight_**

> Just like sleeping in, breaking your diet, or procrastinating on your work, we reap the rewards of thinking in soldier mindset right away, while the costs don’t come due until later. *(15.20910%)*

**_Highlight_**

> Just like the lies we tell others, the lies we tell ourselves have ripple effects.  *(15.58940%)*

**_Highlight_**

> 	 This is another way in which our intuition about costs and benefits is skewed—we overestimate the importance of how we come across to other people. Social costs like looking weird or making a fool out of ourselves feel a lot more significant than they actually are. *(16.34980%)*

**_Highlight_**

> If you were born fifty thousand years ago, you were more or less stuck with the tribe and family you were born into. There wasn’t much in the way of career choice, either. You could hunt, forage, or have children, depending on your role in the tribe. If you didn’t like it, well, that was too bad. We have far more options now. Especially if you live in a relatively developed country, you have the freedom to choose where to live, what career to pursue, who to marry, whether to start or end a relationship, whether to have children, how much to borrow, where to invest, how to manage your physical and mental health, and much more. *(17.11030%)*

**_Highlight_**

> Living in the modern world also means we have many more opportunities to fix things we don’t like about our lives. *(17.11030%)*

**_Highlight_**

> 	 This abundance of opportunity makes scout mindset far more useful than it would have been for our ancestors. After all, what’s the point of admitting your problems exist if you can’t fix them? What’s the point of noticing your disagreements with your community if you can’t leave? Having an accurate map doesn’t help you very much when you’re allowed to travel only one path.
>  *(17.49050%)*

**_Highlight_**

> So if our instincts undervalue truth, that’s not surprising—our instincts evolved in a different world, one better suited to the soldier. Increasingly, our world is becoming one that rewards the ability to see clearly, especially in the long run; a world in which your happiness isn’t nearly as dependent on your ability to accommodate yourself to whatever life, skills, and social groups you happened to be born into.
>  *(17.49050%)*

## Developing Self-Awareness 

**_Highlight_**

> viewing yourself as rational can backfire. The more objective you think you are, the more you trust your own intuitions and opinions as accurate representations of reality, and the less inclined you are to question them. *(18.25100%)*

**_Highlight_**

> 	 A key factor preventing us from being in scout mindset more frequently is our conviction that we’re already in it.  *(18.25100%)*

### Signs of a Scout 

**_Highlight_**

> 	 Language like this seems to imply that the problem with our discourse—the reason so many people have the “wrong” views on controversial topics—is a lack of knowledge and reasoning ability. If only people were smarter and more well-informed, they would realize their errors!
>  *(18.63120%)*

**_Highlight_**

> . When you start from the premise that you’re an objective thinker, you lend your conclusions an air of unimpeachability they usually don’t deserve.
>  *(18.63120%)*

**_Note_**

> If knowledge and intelligence protect you from motivated reasoning, then we would expect to find that the more people know about science, the more they agree with each other about scientific questions. Kahan found the opposite. At the lowest levels of scientific intelligence, there’s no polarization at all—roughly 33 percent of both liberals and conservatives believe in human-caused global warming. But as scientific intelligence increases, liberal and conservative opinions diverge. By the time you get to the highest percentile of scientific intelligence, liberal belief in human-caused global warming has risen to nearly 100 percent, while conservative belief in it has fallen to 20 percent.5 The same funnel-shaped pattern shows up when you ask people for their opinions on other ideologically charged scientific issues: Should the government fund stem cell research? How did the universe begin? Did humans evolve from lower animal species? On all these questions, the people with the highest levels of scientific intelligence were also the most politically polarized in their opinions.6 *(19.01141%)*

* **Comments**: Shit. This is a really interesting finding.

**_Highlight_**

> Intelligence and knowledge are just tools. You can use those tools to help you see the world clearly, if that’s what you’re motivated to do. Or you can use them to defend a particular viewpoint, if you’re motivated to do that instead. *(19.77190%)*

**_Highlight_**

> 1. Do you tell other people when you realize they were right?
>  *(20.15210%)*

**_Highlight_**

> In the rest of this chapter, we’ll explore five signs of scout mindset, behavioral cues that someone cares about truth and will seek it out even when they’re not forced to, and even when the truth isn’t favorable to them.
>  *(20.15210%)*

**_Highlight_**

> examine your track record. Are there examples of criticism you’ve acted upon? Have you rewarded a critic (for example, by promoting him)? Do you go out of your way to make it easier for other people to criticize you?
>  *(20.53230%)*

**_Highlight_**

> 2. How do you react to personal criticism?
>  *(20.53230%)*

**_Highlight_**

> a willingness to say “I was wrong” to someone else is a strong sign of a person who prizes the truth over their own ego. Can you think of cases in which you’ve done the same?
>  *(20.53230%)*

**_Highlight_**

> 	 3. Do you ever prove yourself wrong?
>  *(20.91250%)*

**_Highlight_**

> 	 4. Do you take precautions to avoid fooling yourself?
>  *(21.29280%)*

**_Highlight_**

> Can you think of any examples in which you voluntarily proved yourself wrong?  *(21.29280%)*

**_Highlight_**

> Do you try to avoid biasing the information you get? For example, when you ask your friend to weigh in on a fight you had with your partner, do you describe the disagreement without revealing which side you were on, so as to avoid influencing your friend’s answer? *(21.67300%)*

**_Highlight_**

> Can you name people who are critical of your beliefs, profession, or life choices who you consider thoughtful, even if you believe they’re wrong?  *(22.43350%)*

### Noticing Bias 

**_Highlight_**

> One of the essential tools in a magician’s tool kit is a form of manipulation called forcing. In its simplest form, forcing works like this: The magician places two cards facedown in front of you. In order to make his trick succeed, he needs you to end up with the card on the left. He says: “Now we’re going to remove one of these cards—please pick one.”
>  *(22.81370%)*

**_Highlight_**

> Suppose a Democratic politician gets caught cheating on his wife, but a Democratic voter doesn’t consider that a reason not to vote for him: “What he does in his private life is his own business,” she reasons. However, if the adulterous politician had been a Republican, she would instead have thought, “Adultery is a sign of poor character—that shows he’s not fit to govern.” If the Democratic voter could see the way she would have reacted in that counterfactual world and compare it to her reaction in the actual world, the influence of her motivations would be obvious to her. But because she only ever sees one of those worlds, she never realizes that she’s being anything less than dispassionate. *(22.81370%)*

**_Highlight_**

> If you point to the card on the left, he says, “Okay, that one’s yours.” If you point to the card on the right, he says, “Okay, we’ll remove that one.” Either way, you end up holding the card on the left, feeling like you chose it of your own free will. If you could see both of those possible scenarios at once, the trick would be obvious. But because you end up in only one of those worlds, you never realize.
>  *(22.81370%)*

**_Highlight_**

> Over the next few pages, we’ll explore five different types of thought experiments: the double standard test, the outsider test, the conformity test, the selective skeptic test, and the status quo bias test. *(23.19390%)*

**_Highlight_**

>  the “double standard test”: “Am I judging other people’s behavior by a standard I wouldn’t apply to myself?”  *(23.57410%)*

**_Highlight_**

> thought experiment when I want to test how much of “my” opinion is actually my own. If I find myself agreeing with someone else’s viewpoint, I do a conformity test: Imagine this person told me that they no longer held this view. Would I still hold it? Would I feel comfortable defending it to them?
>  *(24.71480%)*

**_Highlight_**

>  outsider test: Imagine someone else stepped into your shoes—what do you expect they would do in your situation? When you’re making a tough decision, the question of what to do can get tangled up with other, emotionally fraught questions like, “Is it my fault that I’m in this situation?” or “Are people going to judge me harshly if I change my mind?” The outsider test is designed to strip away those influences, leaving only your honest guess about the best way to handle a situation like the one you’re in. *(24.71480%)*

**_Highlight_**

> the selective skeptic test: Imagine this evidence supported the other side. How credible would you find it then? Suppose someone criticizes a decision your company made, and your knee-jerk reaction is, “They don’t know what they’re talking about, because they don’t have all the relevant details.” Selective skeptic test: Imagine the person had praised your company’s decision instead. Would you still think that only insiders are informed enough to have valid opinions? *(25.47530%)*

**_Highlight_**

> the status quo bias test: Imagine your current situation was no longer the status quo. Would you then actively choose it? If not, that’s a sign that your preference for your situation is less  *(25.85550%)*

**_Highlight_**

> Thought experiments aren’t oracles. They can’t tell you what’s true or fair or what decision you should make. If you notice that you would be more forgiving of adultery in a Democrat than a Republican, that reveals you have a double standard, but it doesn’t tell you what your standard “should” be.  *(26.23570%)*

### How Sure Are You? 

**_Highlight_**

> 	 Your strength as a scout is in your ability to resist that temptation, to push past your initial judgment, and to think in shades of gray instead of black and white. To distinguish the feeling of “95% sure” from “75% sure” from “55% sure.” That’s what we’ll learn to do in this chapter.
>  *(27.37640%)*

**_Highlight_**

> What you’re implicitly aiming for when you tag your beliefs with various confidence levels is perfect calibration. That means your “50% sure” claims are in fact correct 50 percent of the time, your “60% sure” claims are correct 60 percent of the time, your “70% sure” claims are correct 70 percent of the time, and so on.
>  *(27.37640%)*

**_Highlight_**

> The word bet might conjure up horse races and blackjack tables, but its meaning is far more general. A bet is any decision in which you stand to gain or lose something of value, based on the outcome. *(29.65780%)*

**_Highlight_**

> So when you’re thinking about how sure you are, your answer will be more honest if you switch from thinking in terms of “What can I get away with claiming to myself?” to “How would I bet, if there was something at stake?”
>  *(29.65780%)*

**_Highlight_**

> I imagine facing a choice between two possible bets. I use a technique I adapted from decision-making expert Douglas Hubbard called an “equivalent bet test.”12 Here’s how it works in this case: I can bet on self-driving cars, and get $10,000 if they’re on the market in a year. Alternately, I can take the “ball bet”: I’m given a box containing four balls, one of which is gray. I reach in and pull out one ball, without looking—if it’s gray, I win $10,000.* *(30.03800%)*

**_Highlight_**

> If you believe “I was being reasonable in that fight with my partner, and he was being unreasonable,” a hypothetical test might go something like this: Suppose another person, an objective third party, is given all of the relevant details about the fight, and is asked to judge which of you two is being more reasonable. If he judges in your favor, you win $1,000; if not, you lose $1,000. How confident do you feel that you would win that bet?
>  *(30.03800%)*

**_Highlight_**

> the fact that I feel more confident in the ball bet implies that I’m less than 25 percent confident in self-driving cars making it to the market in a year.
>  *(30.41830%)*

**_Highlight_**

> There’s a core skill in this chapter, too: being able to tell the difference between the feeling of making a claim and the feeling of actually trying to guess what’s true. Making a claim feels like your press secretary is speaking. It feels pat; neat and tidy. Sometimes hurried, as if you’re trying to put something past yourself. The mental motion is declaring, proclaiming, insisting, or perhaps scoffing. Trying to guess what’s true feels like being the board of directors, deciding how to bet. There’s at least a second or two when you don’t know what answer you’re going to end up giving. It’s like you’re squinting at the evidence, trying to summarize what you see. The mental motions involved are estimating, predicting, weighing, and deliberating. *(30.79850%)*

### Coping with Reality

**_Highlight_**

> One of the most fundamental human needs is to feel like things are basically okay: that we’re not failures, that the world isn’t a horrible place, and that whatever life throws at us, we’ll be able to handle it.  *(31.93920%)*

**_Highlight_**

> “When one has just had a door slammed in one’s face by an angry homemaker, the thought that ‘she was an awful woman’ is clearly superior to ‘I am an inept salesperson.’”6 But are those really our only two options? We could instead tell ourselves, “Yes, I screwed up that sale. But everyone makes mistakes.” Or “Yes, I screwed up that sale. Still, I’m improving—I used to get doors slammed in my face every day, and now it only happens every week!” Surely we can find a way to bounce back from our setbacks that doesn’t require us to blame them on other people—an honest coping strategy. *(32.31940%)*

**_Highlight_**

> People generally take for granted that coping requires self-deception, and experts are no exception. *(32.31940%)*

**_Highlight_**

> But is it really true that we need self-justification to prevent us from “torturing ourselves with regret”? Couldn’t we just . . . learn to not torture ourselves with regret instead? *(32.31940%)*

**_Highlight_**

> 	 Scouts aren’t invulnerable to fear, anxiety, insecurity, despair, or any of the other emotions that give rise to motivated reasoning, and they rely on coping strategies just like anyone else. They just take more care to select coping strategies that don’t mess with the accuracy of their judgment.
>  *(32.69960%)*

**_Highlight_**

>  there are also honest coping strategies, like coming up with a hypothetical plan.
>  *(33.07980%)*

**_Highlight_**

> When a negative emotion strikes, it’s as if we hurriedly reach into the bucket to grab something, anything, to make ourselves feel better. We don’t pay much attention to the kind of coping strategy we pull out, and whether it involves self-deception or not. As long as it makes us feel better, and it’s halfway plausible, it’ll do. What I’m proposing in this chapter is that there is an abundance of different coping strategies, and you don’t need to be so quick to go with the first thing you happen to pull out of the bucket. You can almost always find something comforting that doesn’t require self-deception *(33.07980%)*

**_Highlight_**

> Sometimes when I’m in the middle of an argument, I start to get a sneaking suspicion that I might be in the wrong. That’s not exactly a comfortable prospect. It’s tempting to push the thought out of my mind and focus on saving face. Instead, I remind myself of a silver lining: Conceding an argument earns me credit. It makes me more credible in other cases, because I’ve demonstrated that I don’t stick to my guns just for the sake of it. It’s like I’m investing in my future ability to be convincing. *(33.46010%)*

**_Highlight_**

> 	 It’s striking how much the urge to conclude “That’s not true” diminishes once you feel like you have a concrete plan for what you would do if the thing were true. It doesn’t have to be elaborate. Even a simple plan, like “Here’s how I would explain the failure to my team . . .” or “Here’s how I would begin my search for a new job . . .” goes a long way toward making you feel like you don’t need to rely on denial to cope with reality. *(33.46010%)*

**_Highlight_**

> Call this the self-belief model of success: If you convince yourself that you will succeed, you’ll be motivated to attempt hard things and persist in the face of setbacks, such that eventually your optimism will  *(35.74140%)*

**_Highlight_**

> be self-fulfilling. *(35.74140%)*

### 	 Motivation Without Self-Deception 

**_Highlight_**

> ” makes it sound like everyone has one and only one dream, most people have more than one thing they enjoy and are good at, or could at least become good at. *(36.50190%)*

**_Highlight_**

> This is the biggest problem with the self-belief approach to motivation. Because you’re not supposed to think realistically about risk, it becomes impossible to ask yourself questions like, “Is this goal desirable enough to be worth the risk?” and “Are there any other goals that would be similarly desirable but require less risk?” It implicitly assumes that you don’t need to make any decisions; that you’ve already found the one right path, and there are no other options out there worth weighing.
>  *(36.50190%)*

**_Highlight_**

> If you have an accurate picture of the odds, how do you keep from getting discouraged? How do you motivate yourself to give it your all, while knowing there’s a significant chance your “all” won’t be enough in the end?
>  *(37.64260%)*

**_Highlight_**

> Musk’s low expectation of success confounds people because they assume the only reason to do something is if it’s likely to succeed. But scouts aren’t motivated by the thought, “This is going to succeed.” They’re motivated by the thought, “This is a bet worth taking.” Most people are already on board with the idea of a “bet worth taking,” in at least some contexts. To give a simple example, suppose someone offered you a bet in which you roll a normal six-sided die. If it lands on a six, you win $200; if not, you lose $10. Should you take it? Almost certainly. This is a good bet for you—and you can see exactly how good it is by calculating its expected value. That’s the average amount a bet pays out each time, if you were to take it an infinite number of times. *(38.02280%)*

**_Note_**

> It might be motivating to believe with absolute certainty that you’re going to win, but it’s not realistic—there’s always some element of chance involved, in any endeavor. Over time, your outcomes will fluctuate; some of your bets will turn out well, and many will turn out poorly. But as long as you continue making positive expected value bets, that variance will mostly wash out in the long run. Building that variance into your expectations has the nice side effect of giving you equanimity. Instead of being elated when your bets pay off, and crushed when they don’t, your emotions will be tied to the trend line underneath the variance. The goal isn’t to attribute everything to luck. It’s to do your best to mentally separate out the role that luck plays in your results from the role that your decision-making plays, and to judge yourself based on the latter. *(39.16350%)*

* **Comments**: Seeing the upward trend in daily fluctuations keeps you motivated.

**_Highlight_**

> You want to get into a mental state where if the bad outcome comes to pass, you will only nod your head and say ‘I knew this card was in the deck, and I knew the odds, and I would make the same bets again, given the same opportunities. *(39.92400%)*

**_Highlight_**

> accepting the possibility of failure in advance is liberating. It makes you bold, not timid. It’s what gives you the courage to take the risks required to achieve something big.
>  *(39.92400%)*

**_Highlight_**

> Instead of being motivated by the promise of guaranteed success, a scout is motivated by the knowledge that they’re making a smart bet, which they can feel good about having made whether or not it succeeds. Even if a particular bet has a low probability of success, they know that their overall probability of success in the long run is much higher, as long as they keep making good bets. They’re motivated by the knowledge that downturns are inevitable, but will wash out in the long run; that although failure is possible, it’s also tolerable.
>  *(40.30420%)*

### Influence Without Overconfidence 

**_Highlight_**

> 	 This would seem to bode poorly for scouts; if you’re being intellectually honest, you’re not going to have certainty about everything. *(40.68440%)*

**_Highlight_**

> 	 The common wisdom is that the more confidence you can muster in your beliefs, the more influential you will be. Confidence is magnetic. It invites people to listen to you, follow you, and trust that you know what you’re doing.  *(40.68440%)*

**_Highlight_**

> Separately, there’s social confidence, or self-assurance: Are you at ease in social situations? Do you act like you deserve to be there, like you’re secure in yourself and your role in the group? Do you speak as if you’re worth listening to?
>  *(41.06460%)*

**_Highlight_**

> Confidence is one of those words that we use to mean different things without even realizing it. One is epistemic confidence, or certainty—how sure you are about what’s true.  *(41.06460%)*

**_Highlight_**

> the researchers screened the videos for other people and asked them: “How competent does each of these students seem to you?” The competence ratings the students received were predominantly based on how much social confidence they displayed. The more a student participated in conversation, used an authoritative tone of voice, and had a relaxed demeanor, the more competent they seemed. By comparison, the students’ epistemic confidence hardly mattered. Their statements about how certain they were in their answer, how easy the task was for them, or how competent they were at the task were barely or not at all significant.
>  *(41.82510%)*

**_Highlight_**

> When people claim that “admitting uncertainty” makes you look bad, they’re invariably conflating these two very different kinds of uncertainty: uncertainty “in you,” caused by your own ignorance or lack of experience, and uncertainty “in the world,” caused by the fact that reality is messy and unpredictable. The former is often taken as a bad sign about someone’s expertise, and justifiably so. But the latter is not *(42.58560%)*

**_Highlight_**

> if you show that certainty is unrealistic, you can be more  *(42.96580%)*

**_Highlight_**

> Even if reality is messy and it’s impossible to know the right answer with confidence, you can at least be confident in your analysis. *(42.96580%)*

**_Highlight_**

> 	 One reason people don’t like hearing uncertain answers is that it leaves them at a loss for how to act. You can reassure them by following up your uncertainty with a plan or recommendation.
>  *(43.34600%)*

## Changing Your Mind

### How to Be Wrong 

**_Highlight_**

> 	 Changing your mind frequently, especially about important beliefs, might sound mentally and emotionally taxing. But, in a way, it’s less stressful than the alternative. If you see the world in binary black-and-white terms, then what happens when you encounter evidence against one of your beliefs? The stakes are high: *(46.00760%)*

**_Highlight_**

> superforecasters are much happier to think about what they got wrong—they know that analyzing their errors is an opportunity to hone their technique. *(46.38780%)*

**_Highlight_**

> one of the biggest benefits of noticing your errors: the opportunity to improve your judgment in general.  *(46.76810%)*

**_Highlight_**

> 	 Mark’s implicit assumption was that changing your mind is humbling. That saying “I was wrong” is equivalent to saying “I screwed up”—something you confess with contrition or sheepishness. Indeed, that’s the standard way of thinking about being wrong. *(47.52850%)*

**_Highlight_**

> most of the time, being wrong doesn’t mean you did something wrong. It’s not something you need to apologize for, and the appropriate attitude to have about it is neither defensive nor humbly self-flagellating, but matter-of-fact.
>  *(47.90870%)*

**_Highlight_**

> An update is routine. Low-key. It’s the opposite of an overwrought confession of sin. An update makes something better or more current without implying that its previous form was a failure.
>  *(48.28900%)*

**_Note_**

> “My intuition is that if you bring up these biases a lot—that we are always going to think that we’re right, that we’re always thinking what we’re doing is the best and most important thing to be doing . . . it makes it an easier pill to swallow when inevitably something better comes along. Because you’ve kind of inoculated yourself against the ‘horror’ of having been suboptimal for some  *(49.04940%)*

* **Comments**: Basically, just be ready for the very real possibility that you could be wrong.

### Lean In to Confusion 

**_Highlight_**

> That’s the thing about surprising and confusing observations. You don’t know in advance what they’ll teach you. All too often, we assume the only two possibilities are “I’m right” or “The other guy is right”—and since the latter seems absurd, we default to the former. But in many cases, there’s an unknown unknown, a hidden “option C,” that enriches our picture of the world in a way we wouldn’t have been able to anticipate.
>  *(52.09130%)*

**_Highlight_**

> He calls it a “de minimus error,” an attempt to minimize the inconsistency between observations and theory.11 Each new piece of evidence that doesn’t fit a doctor’s medical diagnosis can be explained away or dismissed as a fluke, so the doctor never realizes her initial diagnosis was wrong. *(53.61220%)*

**_Note_**

> 	 It’s a tricky skill. It forces you to act without clarity, to operate under one paradigm while being aware of its flaws and inconsistencies, knowing that it might be wrong and that you might end up abandoning it. You have to resist the temptation to resolve inconsistency prematurely by forcing all of your observations into one paradigm, and instead be willing to remain confused—for days, weeks, or even years.
>  *(53.99240%)*

* **Comments**: Being okay with staying confused and resisting the temptation to resolve inconsistencies... Never thought of the payoff of that before. Makes a lot of sense.

**_Highlight_**

> What the best decision-makers do is look for ways to make sense of conflicting evidence under their existing theory but simultaneously take a mental note: This evidence stretches my theory by a little (or a lot). If your theory gets stretched too many times, then you admit to yourself that you’re no longer sure what’s happening, and you consider alternate explanations. *(53.99240%)*

**_Highlight_**

> Scouts view anomalies as puzzle pieces to collect as you go through the world. You probably won’t know what to do with them at first. But if you hang on to them, you may find that they add up to a richer picture of the world than you had before. *(54.37260%)*

### Escape Your Echo Chamber 

**_Highlight_**

> One month later, researchers measured the political attitudes of their participants. Had their views been moderated by the foray outside their echo chambers? Quite the contrary. Conservatives who spent a month reading liberal tweets became dramatically more conservative. Liberals who spent a month reading conservative tweets became slightly more liberal *(55.13310%)*

## Rethinking Identity 

### How Beliefs Become Identities

**_Highlight_**

> Beliefs crystallize into identities through the feeling of being under siege from a hostile world, *(60.45630%)*

**_Highlight_**

> Beliefs also become part of your identity when they come to represent some virtue you take pride in. *(60.83650%)*

**_Highlight_**

> Prefacing a claim with the phrase “I believe” is a tip-off that it’s important to your identity. *(61.97720%)*

**_Highlight_**

> When you feel the urge to step in and defend a group or belief system against perceived criticism, chances are good that your identity is involved. *(62.35740%)*

**_Highlight_**

> . A righteous tone You may have noticed the righteous flourishes we sometimes add to the ends of our statements: Period. Full Stop. End of story. End of discussion. It’s that simple. Or how about the currently popular practice of breaking up a sentence with an emphatic period after every word? You don’t support this policy? You. Are. Part. Of. The. Problem. *(62.73760%)*

**_Highlight_**

> Proud, standing up, unapologetic, fearless—defiant language like this is a sign that you see yourself as an embattled minority viewpoint facing off against a society *(62.73760%)*

**_Highlight_**

> When a label is more than just a practical description of your beliefs—when it feels like a status symbol or a source of pride—then the question of who else gets to wear that label actually matters. It becomes important to police the identity’s boundaries.
>  *(62.73760%)*

**_Highlight_**

> Deriving pleasure from news that humiliates some ideological group you disagree with is a sign of an “oppositional identity”—an identity defined by what it opposes. *(63.11790%)*

**_Note_**

> The problem with identity is that it wrecks your ability to think clearly. Identifying with a belief makes you feel like you have to be ready to defend it, which motivates you to focus your attention on collecting evidence in its favor. Identity makes you reflexively reject arguments that feel like attacks on you or the status of your group. It turns empirical questions such as “How large are the health benefits of breastfeeding?” into questions that are much more emotionally fraught and difficult to think clearly about: “Am I a good mother? Am I a good feminist? Will my friends judge me? Was ‘my side’ vindicated or humiliated?” And when a belief is part of your identity, it becomes far harder to change your mind, even when the facts change dramatically. *(63.49810%)*

* **Comments**: Basically, don't get too attached to your beliefs if you want to be able to change them faster.

**_Highlight_**

> 	 If you use epithets like these in talking about a particular issue, that’s a sign you’re viewing it as a fight between people, not ideas. That doesn’t necessarily mean your position on the issue is wrong or that the other side is right, but it does mean that your identity is probably coloring your judgment.
>  *(63.49810%)*

### 	 Hold Your Identity Lightly 

**_Highlight_**

> Holding an identity lightly means thinking of it in a matter-of-fact way, rather than as a central source of pride and meaning in your life. It’s a description, not a flag to be waved proudly.
>  *(64.25860%)*

**_Highlight_**

> It means maintaining a sense of your own beliefs and values, independent of the tribe’s beliefs and values, and acknowledging—at least in the privacy of your own head—the places where those two things diverge.
>  *(64.63880%)*

**_Highlight_**

> 	 That’s now called the Turing test. The ideological Turing test, suggested by economist Bryan Caplan, is based on similar logic.11 It’s a way to determine if you really understand an ideology: Can you explain it as a believer would, convincingly enough that other people couldn’t tell the difference between you and a genuine believer?
>  *(65.39920%)*

**_Highlight_**

> The better your message makes you feel about yourself, the less likely it is that you are convincing anyone else.” *(66.53990%)*

### 	 A Scout Identity 